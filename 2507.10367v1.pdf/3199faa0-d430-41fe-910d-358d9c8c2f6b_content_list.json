[
    {
        "type": "text",
        "text": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Jingwei  $\\mathrm{Xu}^1$  , Junbin Kang?, Mingkai Dongl, Mingyu Liu?, Lu Zhang?, Shaohong Guo?, Ziyan  $Qiu^2$  Mingzhen You', Ziyi Tian', Anqi  $Yu^2$  Tianhong Ding?, Xinwei  $H u^{2}$  and Haibo Chen1,2 Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University 2Huawei Technologies",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "AbstractClient- side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client- side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless- client architecture. Specifically, instead of performing client- side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to  $5.72\\times$  throughput for small file read/write and up to  $12.81\\times$  throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 Introduction",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Distributed file systems (DFSs) are essential components of modern data centers. By providing POSIX- compliant file interfaces within a unified, hierarchical directory structure, DFSs enable general access to underlying storage resources, thereby simplifying storage management and facilitating data sharing among diverse applications [2, 35]. As a result, DFSs form the foundational storage layer for critical data center services, such as block storage and object storage [24,27], as well as for a broad array of applications, including data processing and analysis [38, 42], high- performance computing [25, 52], and artificial intelligence pipelines [34, 46, 51].",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "However, the POSIX interface and tree- structured directory organization of DFSs are a double- edged sword, particularly in the context of deep learning (DL) workloads. On one hand, the POSIX interface is valued for its generality and convenience, facilitating integration with existing applications and frameworks. On the other hand, the POSIX interface and hierarchical directory organization are not well- suited for distributed environments, leading to inefficiencies in many key use cases. Specifically, the tree- structured organization requires frequent path resolution in DFSs. Before any file operation, the DFS client must resolve the complete path to locate the target file's inode, which involves verifying the existence and permissions of every directory along the path (Figure 1). As modern DFSs typically distribute directory metadata across multiple metadata servers for scalability, path resolution entails multiple round- trips between the client and metadata servers. This leads to significant request amplification: a single file operation entails multiple network requests.",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/88079a3a09ad2d44b80a1f1c6a16edc57d0c41e252cdee41fd2fc43ce38f5d38.jpg",
        "image_caption": [
            "Figure 1: Path resolution in a typical distributed file system operation. The client checks path existence and permission by looking up each path component, which may involve multiple remote requests."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/5f7a0442652b5bec8b09bc17fcc8d08c27116159918bb77c555d23d05fcd3581.jpg",
        "image_caption": [
            "Figure 2: CephFS performance of randomly traversing 64 KiB files in a large directory tree under different client metadata cache sizes. The left y-axis represents the read throughput and the right y-axis represents the number of requests sent to the MDSs (i.e., lookup and close). The dashed line denotes the file number."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To mitigate path resolution overhead, existing DFSs employ client- side metadata caching, where clients maintain a local cache of directory/file metadata to avoid frequent remote lookups [4, 14, 15, 25, 27, 34, 36, 37, 46, 48]. We refer to clients with client- side caching as stateful clients, as they maintain metadata state locally. By caching resolved paths, stateful clients reduce network round- trips for previously resolved",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "and locally cached file operations, improving performance.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "However, this stateful- client architecture is inefficient for DL training workloads. Unlike general- purpose workloads that exhibit strong locality and thus achieve high path resolution cache hit rates, DL training workloads demonstrate multiple traversals of massive directory trees, containing billions of directories and hundreds of billions of files in production environments, all accessed in random order.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "When accessing such massive directory structures, the stateful- client architecture faces an inherent trade- off: either consume a huge amount of client memory to cache the large directory tree or suffer a severe performance drop due to request amplification. To present this trade- off, we evaluate how the metadata cache size affects the performance of random file traversal in a large directory tree, as shown in Figure 2 (further explained in  $\\S 2.3$ ). Compared to a cache that stores all directories, a cache only  $10\\%$  as large results in a  $32.5\\%$  throughput reduction, primarily because the lookup requests increase by  $1.50\\times$ . The dilemma is further exacerbated by the fact that even caching  $10\\%$  of the directories is prohibitively expensive in production, considering the large scale directory tree and the client number. Even when  $90\\%$  of directories are cached, an average of 1.70 network hops (i.e., the lookup requests in the figure) are required to complete a file open.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In this paper, we propose the stateless- client architecture, which achieves one- hop access for most file operations in DL workloads while requiring no client- side caching. The core difference is that stateless client shifts path resolution to the server side. However, to realize the stateless- client architecture for an efficient DFS, two key problems remain to be addressed. First, to enable one- hop access for most file operations, the client needs an approach to finding the correct metadata server that stores the target file's inode. Second, as the path resolution is now on the server side, each metadata server should be able to resolve path locally, without involving additional network hop when processing client's file operation request.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We realize the stateless- client architecture in FalconFS, a POSIX- like high- performance DFS designed for DL workloads. FalconFS proposes hybrid metadata indexing to address the first key problem of locating the appropriate metadata server (\\$4.2). This approach leverages filename hashing to place file inodes to metadata servers while achieving load balance with selective redirections.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "To address the second problem of resolving path locally, FalconFS proposes lazy namespace replication, which replicates the filesystem's whole namespace (i.e., the directory tree) to all metadata servers (\\$4.3). To reduce maintenance overhead, modifications to the namespace are lazily synchronized and an invalidation- based mechanism is adopted for concurrency control.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "FalconFS further adopts concurrent request merging to amortize operation overhead and improve concurrency (\\$4.4) and VFS shortcut to keep the stateless- client architecture com patible with Linux VFS for easy deployment (\\$5).",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/5c84d9d0b191adceb46b5c7576fc4c6f654adb3c9340562e333b509ed9c76b7e.jpg",
        "image_caption": [
            "Figure 3: Overview of a DL Pipeline for Autonomous Driving."
        ],
        "image_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Evaluations show that FalconFS completely eliminates request amplification and improves performance of large directory tree random traversal by up to  $4.72\\times$  over CephFS [48] and up to  $3.34\\times$  over Lustre [4]. End- to- end experiments show that FalconFS improves the throughput of DL model training by up to  $11.81\\times$  and  $1.23\\times$  over CephFS and Lustre, respectively. FalconFS has been deployed in Huawei's AI clusters with 10,000 NPUs for data labeling and model training of the autonomous driving solution for one year.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Our main contributions are as follows.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "- Analyzing IO characteristics of deep learning workloads, pointing out the inefficiency of the traditional stateful-client architecture.- Proposing the stateless-client architecture and addressing the challenges of designing an efficient DFS.- Building FalconFS, a high-performance DFS designed for DL workloads and a thorough evaluation on it.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 DL Pipelines: IO Patterns and Challenges",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1 Deep Learning Pipeline",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Deep learning (DL) models are widely employed in autonomous driving, computer vision, and big data analysis. To meet evolving quality demands, training pipelines have been developed to continuously retrain models using large, iteratively updated datasets.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Figure 3 illustrates the architecture of the deep learning (DL) training pipeline employed in Huawei's autonomous driving (AD) systems. The pipeline consists of four key stages: The ingestion stage collects raw data from real- world environments. The labeling stage generates labels for the raw data with a sequence of model inference tasks, including moving objects detection, lane detection, traffic sign detection, etc. Then, the training stage uses a labeled dataset to train the target model for vehicle deployment. In the final archiving stage, the dataset is moved into low- cost storage systems such as cloud data lakes for future reference [16,30,49,51]. Similar architectures also exist in other DL workloads [1,31,51].",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.2 Workload Patterns in DL pipelines",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We take the DL pipeline for autonomous driving as an example to present unique workload patterns in DL pipelines.",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Enumerous small objects in large directories. The autonomous driving pipeline consumes multimodal data, including images, point clouds, etc. During labeling and training, these objects are stored as individual files, whose size ranges from a few KiB to a few MiB, mostly within 256KiB. In production, an inflight dataset scales up to hundreds of petabytes and is composed of over 300 billion small files. These files are grouped into directories by timestamps, vehicle ID, camera ID, etc., forming a directory tree with billions of directories and large directory sizes. The huge number of files and directories stresses the DFS's metadata scalability and performance.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Random file traversal. In the training stage, tasks access the dataset in a traversal and random manner. In particular, each file is accessed exactly once in each training epoch, and the access order is random. Such a random access pattern is unfriendly to client caching, which we further discuss in  $\\S 2.3$",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Burst file access. In the labeling stage, the inference tasks read and write files from/to FalconFS in a pipeline, involving massive small file IO and directory lists. To fully utilize GPU's parallelism, data objects are accessed and processed in batches, resulting in burst file access in the same directory, which can lead to instantaneous load imbalance on the metadata servers and downgrade the performance, which we further discuss in  $\\S 2.4$",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Tight Resource budget. CPU and memory are scarce resources for computing nodes. Training tasks perform data augmentation on CPUs, consuming significant CPU cycles and memory resources. In production, CPU for data augmentation is often the bottleneck, and it is typical to store and reuse intermediate results in memory to reduce CPU load [16, 22]. Therefore, the resources available to DFS clients are limited.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.3 Challenge 1:Lookup Tax",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "For small- file intensive workloads like DL pipelines, metadata performance often becomes the bottleneck. To accelerate metadata operations, most DFSs employ a stateful client architecture that caches metadata on the client side [4, 14, 15, 25, 27, 34, 36, 37, 46, 48]. However, this approach is inefficient for DL training workloads, as their large directory working sets are inefficient to cache, leading to the following dilemma.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Challenge 1: Large working sets in deep learning training tasks create a dilemma between performance degradation due to request amplification and excessive client memory consumption required for caching directory metadata.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "To illustrate this dilemma, we replay a trace of Resnet50 model training on a small dataset, comprising 10 million 64KiB files in 1 million 7- level directories, stored in a CephFS instance with 4 MDSs and 12 OSDs A total of 512 IO threads iterate through the files in random order. As shown in Figure 2, the client cache size significantly impacts both read throughput and the number of remote requests generated.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The experiment presents that the read throughput is highly sensitive to the client's metadata cache size. Compared to caching  $10\\%$  directories, caching all directories achieves  $1.46\\times$  higher throughput. Furthermore, throughput increases significantly as the cache size grows from  $10\\%$  to  $100\\%$  ,indicating that optimal performance requires allocating sufficient memory to cache all directories. Otherwise, performance degrades proportionally with reductions in cache size.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The DL training workload's sensitivity to cache size comes from its random access nature. During each epoch, training tasks access every data object exactly once in a random order. When all directories are cached, all directory lookups are served by the cache, reducing each file open operation to a single metadata lookup request. However, with a small cache size, the LRU policy preferentially retains near- root directories, while the hit rate of last- level directories - which constitute  $90\\%$  of accesses in the experiment - is proportional to the cache size. As shown in Figure 2, smaller caches correlate with increased lookup requests due to cache misses. This effect causes request amplification: each file open operation triggers multiple lookup requests to the metadata server, bottlenecking read throughput.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In production environments, caching a significant portion of working- set directories proves prohibitively expensive. For instance, a production cluster in Huawei can scale over 1000 client nodes and a typical production dataset contains billions of directories. Given that in Linux VFS, caching a directory takes 800 bytes (608 bytes for inode and 192 bytes for dentry), caching  $10\\%$  of 1 billion directories on each node would require  $80\\mathrm{GiB}$  per node and 80 TiB in total, which is prohibitively expensive.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.4 Challenge 2: Burst IO",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "During the labeling stage, inference tasks load and store data objects to working directories using batched file operations. This batched IO pattern causes instantaneous load imbalances, which limit performance scalability. We illustrate this issue by performing burst file access on a CephFS cluster with 4 MDSs and 12 OSDs and present the results in Figure 4(a).",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We observe that when the burst size exceeds the IO parallelism of the tasks, the DFS's performance degrades. Analysis of load distribution (Figure 4(b)) shows that at a burst size of 100, the MDSs experience severe load imbalance during read operations. This issue arises because CephFS tends to store metadata for files within the same directory together; consequently, burst operations on files in the same directory can congest a single MDS, leading to performance bottlenecks. Similar patterns also exist in other DFSs like [15, 27, 34, 36, 46].",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Challenge 2: Concurrent operations on files within the same directory suffer from MDS congestion, hindering performance scalability.",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/4441272a2796112b83f70e600d43ce5cf76cc3c075fd92042ff687aa98ef0b84.jpg",
        "image_caption": [
            "Figure 4: CephFS performance of accessing 64KiB files with different burst size. Large burst size leads to load imbalance and performance degradation. Figure 4(b) shows the load variance of the MDSs when performing read operations with burst size 100."
        ],
        "image_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 Proposal: DFS with Stateless Client",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Given the challenges brought by DL workloads, we propose the stateless- client architecture for DFS under DL workloads. The stateless- client architecture abandons the cache on the client side and moves path resolution to the server side. The proposal is based on the following three observations. (1) The server has more memory resources than the clients due to the tight resource budget of the computing node as we described in  $\\S 2.2$  2A typical DL cluster usually has far more clients than metadata servers. In Huawei's deployment environment, the ratio of client machines to server machines exceeds 40:1, and each client machine can host multiple clients. Considering that all clients share the same dataset in DL pipelines, a server- side dentry can serve more clients than a client- side cache. (3) Stateful- client DFSs typically use VFS dcache and inode cache to cache directory attributes [3,4,15,48], which takes 800 bytes for each directory. On the server side, a directory entry can be maintained in a custom format which takes less than 100 bytes. However, to realize the stateless- client architecture for an efficient DFS, two key problems remain.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "How to find the right server? First, the client needs an approach to finding the correct metadata server that stores the target file's inode. Since DFS usually partitions all inodes to multiple metadata servers for scaling out, only the server containing the file's inode can complete its file operations. DFS thus needs to maintain the path- to- server mapping (i.e., the indexing), which is usually implemented in two ways.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Path- walk indexing. Most DFSs use indexing methods that are related to the parent directories, for example, using the parent directory ID as (part of) the key [15,25,27,34,46] for hashing, or indexing files in different directories with different hashing rules [4,14,36]. These approaches require a full- path walk to resolve inode location, and need caching directory entries on clients to achieve better performance, suffering the trade- off in  $\\S 2.3$  Full- path hashing [39,41] determines the target inode location by hashing the full path. It does not require cache, but makes directory rename prohibitively expensive due to the relocation of all inodes in the subtree.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "However, none of these indexing approaches meets of requirements of an efficient DFS with the stateless- client architecture, including no client cache, no extra network hops, and practical support of directory renames. Nevertheless, these approaches inspire us to build a hybrid indexing of hashing and path- walking to solve the problem.",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/cfdff0b924cbfa65b6be959ede630309f7efe4c73bc14995011310563fa3fe67.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/9f9c7095ead83de7df9a862ed5c818a7b48974c5b444c26a14e2ffb9be78a581.jpg",
        "table_caption": [
            "Figure 5: Architecture of FalconFS Table 1: Scheme of FalconFS's metadata."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>Key</td><td>Value</td><td>Partition by</td></tr><tr><td>dentry</td><td>pid, name</td><td>id, perm.</td><td>replicated</td></tr><tr><td>inode</td><td>pid, name</td><td>id, attr</td><td>$4.2</td></tr></table>",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "How to resolve path locally? Second, as the path resolution is now on the server side, each metadata server should be able to resolve the path locally, without involving another network hop when processing the client's file operation request. Since DFS scatters directory entries across multiple metadata servers for scaling out, the dentry information of components in a path is also scattered on multiple servers. A server resolving the path thus needs to communicate with other servers to complete the resolution, which can be expensive.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Considering the characteristics of DL workloads (random file traversal) and DL clusters (high client- server number ratio and rich server memory resource), we thus propose to replicate the whole directory tree (i.e., the namespace) on all metadata servers, so that each server can resolve path locally.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "With hybrid metadata indexing and lazy namespace replication, we build FalconFS with the stateless- client architecture, which eliminates client- side caching while providing one- hop access for most operations in DL workloads.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4 System Design",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4.1 FalconFS Overview",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Architecture. Figure 5 shows the FalconFS architecture, consisting of Client Module, MNode, Coordinator, and File Store.",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "FalconFS's client module is a kernel module that provides POSIX interfaces through Linux VFS. Unlike most Linux file systems, FalconFS client avoids performing path resolution at the client side. Specifically, FalconFS client shortcuts VFS path walk (\\$5) and forwards the operation request with full path to MNodes according to hybrid metadata indexing (\\$4.2).",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/6a79fbcaffc8094b1c4b3c17253ca0d198c35f2a8bdf0349e34e3874c5ff5496.jpg",
        "image_caption": [
            "Figure 6: Hybrid Metadata Indexing."
        ],
        "image_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Metadata nodes (MNode) are PostgreSQL databases [7] with customized extensions. We manage metadata for FalconFS in the extensions using the database's table and transaction management, B- link tree index, xlog (write- ahead logging), and primary- secondary replication. Each MNode maintains a lazily synchronized directory tree structure (namespace replica) and a partition of file attributes (inodes). Leveraging the merits of central location as servers, MNode merges concurrent requests (§4.4) to de- duplicate shareable execution processes for higher throughput.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The central coordinator is responsible for managing namespace changes (§4.3). It also runs a load balancing algorithm to balance inode distribution across MNodes (§4.2).",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "File Store is a distributed block storage storing file data. File blocks are indexed by hashing file ID combined with block offset.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Replicated directory namespace FalconFS replicates the file system directory structure across all MNodes, enabling each MNode to resolve file paths and check permissions locally. The namespace replica contains entries of directory attributes required for path resolution, i.e., dentries in Table 1, and does not include file attributes. With one billion directories, the storage footprint for the namespace replica is less than 100 GiB per MNode, which is acceptable.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Sharded file metadata In contrast to directories, we distribute all the file metadata, i.e., inodes in Table 1, across the metadata servers by hybrid metadata indexing for scaling up metadata capacity and throughput.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4.2 Hybrid Metadata Indexing",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In this section, we present hybrid metadata indexing (§4.2.1) and how to maintain load balance (§4.2.2).",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4.2.1 Hybrid Indexing Methods",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Hybrid metadata indexing adopts filename hashing in the common case. However, for generality, hybrid metadata indexing considers situations where filename hashing may bring uneven inode distribution, and adopts two fallback mechanisms to mitigate the problem. Figure 6 presents an overview.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Common case fast: filename hashing. To accelerate the path resolution, FalconFS adopts filename hashing as the indexing methods for most cases. With filename hashing, all files are placed via the hashing of the filename. Such an indexing method is easy and efficient - it requires no client state and support efficient rename. However, filename hashing cannot guarantee that the files are distributed evenly among different servers, potentially causing load- imbalance issues.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Fortunately, we find that DL workloads' large directory size facilitate the filename hashing, significantly reducing the occurrence of load imbalance. Specifically, DL datasets usually has large directory size, which can be multiple times larger than the number of MNodes. We analyze Huawei's in- production datasets and popular open- sourced datasets that contains more than ten thousand files [11, 13, 17, 26, 44], finding their directory size ranging from several hundreds to hundreds of thousands. According to the law of large numbers, given such large directory sizes, the distribution of files in each directory to MNodes is likely to be uniform. Consequently, the entire namespace is statistically uniformly distributed as the superposition of the distributions for each directory. This forms the basis of using filename hashing as the indexing method for the common cases in DL workloads.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Corner cases correct: selective redirection. However, there are situations where filename hashing can bring uneven file distribution. (a) Hot filenames: The naming convention of applications can cause certain filenames to be more frequent than others. (b) Hash variance: The number of unique file- names is not far more than that of MNodes, which can lead to unbalanced distribution due to hash variance.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To handle these corner cases, FalconFS adopts selective redirection as a complement. For this, FalconFS maintains a shared data structure - exception table, which specifies which and how filenames should be redirected. There are two kinds of redirection, targeting hot filenames and hash variance, respectively.",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "- Path-walk redirection. To place files with hot filenames, FalconFS calculates the hashing value by not only the filename, but also its parent directory ID. Thus, even if a hot filename exists in many directories, the corresponding files are placed on different MNodes. When clients identify a file marked for path-walk redirection in the exception table, they send requests to random MNodes. The receiving node utilizes its local namespace replica to walk and obtain the parent directory ID, and forwards the request to the target node determined by hashing both filename and parent directory ID.- Overriding redirection. When hash variance causes uneven filename distribution across nodes, FalconFS can reassign selected filenames to designated nodes, shifting load from overloaded to underutilized nodes. These placement overriding are maintained in the exception table. Clients encounter",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "tering files marked for such redirection in the exception table send requests directly to their designated MNodes.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "FalconFS maintains copies of the exception table on each MNode, clients and the coordinator. The coordinator updates the exception table according to the scheduling policy detailed in  $\\S 4.2.2$  . Once the exception table is updated, the latest table is eagerly pushed to all MNodes, and clients lazily fetch the updates from MNodes when they get responses to operation requests. The lazy fetching mechanism creates a time window where clients may operate with stale exception tables and directing requests to incorrect nodes. However, MNodes validate all requests by checking its local exception table and forward misdirected requests to the proper destinations.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "A constant number of exception table entries are sufficient to balance the distribution of inodes for arbitrary directory structures. We provide a theoretical analysis in  $\\S \\mathrm{A.1}$",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4.2.2 Statistical load balancing.",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "The coordinator uses the statistics periodically reported by MNodes to make re- balancing decisions.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Statistics. Each MNode periodically reports its local inode count and the most frequent  $O(nlogn)$  local filenames with their occurrence counts, where  $n$  is the number of MNodes.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Load balancing algorithm. The coordinator leverages an algorithm to maintain load balance across nodes. The algorithm aims to keep each node's inode count below  $\\textstyle{\\left(\\frac{1}{n} + \\epsilon\\right)}$  of the total inode count, while minimizing the size of the exception table. e is a parameter specified by the user used to control the effect of load balancing.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "When the coordinator detects load imbalance using statistics reported by MNodes, it conducts the load balancing algorithm as follows.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "1. Identify the most and least loaded nodes. Let  $N_{max}$  and  $N_{min}$  denote the nodes with the highest and lowest inode counts, respectively, where the inode counts are denoted as  $\\langle N_{max}\\rangle$  and  $\\langle N_{min}\\rangle$  \n2. Select the most frequent filename in  $N_{max}$  as  $F$  \n3. Redistribute  $F$  . The coordinator selects from the two redirection methods to approach load balancing. If we use pathwalk redirection, assuming it evenly distributes all files with filename  $F$  to all nodes, the inode numbers of  $N_{max}$  and  $N_{min}$  will be  $\\begin{array}{r}\\langle N_{max}\\rangle - \\frac{n - 1}{n} |F| \\end{array}$  and  $\\begin{array}{r}\\langle N_{min}\\rangle +\\frac{1}{n} |F| \\end{array}$  , respectively, where  $|F|$  represents the number of files named  $F$  in node  $N_{max}$  . If using overriding redirection for  $F$  ,we will transfer all  $|F|$  files from  $N_{max}$  to  $N_{min}$  , yielding the inode numbers of  $\\langle N_{max}\\rangle - |F|$  and  $\\langle N_{min}\\rangle +|F|$  , respectively. The coordinator chooses the method that minimizes the maximum inode count. After the method is chosen, the redirection entry is inserted to the exception table and the corresponding files are migrated among nodes. To ensure metadata consistency, access to the corresponding inodes are temporarily blocked during the migration.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4. Repeat the procedure until no further imbalance is detected.",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/ac03180a59c433e79f23f507c9585ece6127eddb20d3a9e7d5aa305b85ecb27d.jpg",
        "image_caption": [
            "Figure 7: Namespace Synchronization. Blue lines indicate remote lookup and read lines indicate invalidate."
        ],
        "image_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Moreover, the coordinator periodically attempts to shrink the exception table to reduce redirection overhead. Specifically, it iterates all path- walk redirection entries in random order, and removes the entry if removing it does not lead to load imbalance. It then checks and removes overriding redirection entries similarly.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4.3 Lazy Namespace Replication",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Lazy namespace replication is another key enabler of the stateless- client architecture. FalconFS maintains a consistent but not necessarily complete namespace replica on each MNode and the coordinator, enabling local path resolution. To reduce overhead of maintaining consistency across all replicas, modifications to the namespace are lazily synchronized and an invalidation- based mechanism is adopted, inspired by cache coherence protocols and Hermes [20]. The design is guided by two principles:",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Delaying synchronization until access. Directory creation is critical for DL dataset initialization, so the performance is important. Eagerly replicating directory creation across all MNodes would require expensive and unscalable twophase commits (2PC). Instead, FalconFS amortizes the overhead by deferring synchronization until access. Using invalidation as lightweight locking. When operations modify directory structures or permissions, concurrent operations in the sub- tree under the modified directory must be blocked for consistency. We invalidate the corresponding replica entry on all nodes for this instead of using traditional two- phase locking, saving a round of request broadcast.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "We will then introduce how the namespace replicas are maintained during related operations including creating/remove a directory, changing the permissions, and renaming.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Creating a directory. Figure 7(a) shows an example of how to create the directory /b. The client calculates the location of /b via the hybrid metadata indexing, and then sends a mkdir request to the corresponding MNode, i.e., MNodeo. Upon receiving the request, MNodeo resolves the path by querying",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "its local namespace replica to check the path existence and permissions. It also checks its inode table to confirm  $\\mathcal{A}$  does not already exist. Once all checks are passed,  $M N o d e_{0}$  creates an inode for  $\\mathcal{A}$  in the inode table, adds a dentry for  $\\mathcal{A}$  to its local namespace replica, and responds to the client.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Note that to retain the efficient single- hop processing of mkdir,  $M N o d e_{0}$  does not proactively broadcast the new dentry to namespace replicas on other MNodes. Instead, the dentry information is fetched by other namespace replicas on demand. Specifically, when a MNode queries its local namespace replica for path resolution and finds a missing dentry, it fetches the information from the owner MNode calculated via the hybrid metadata indexing.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Figure 7(b) shows an example. When  $M N o d e_{1}$  resolves the path  $\\mathcal{A} / \\mathcal{C}$  it finds the dentry for  $\\mathcal{A}$  missing in its local namespace replica. It then sends a lookup request to  $M N o d e_{0}$  (i.e., the owner MNode) to fetch the missing dentry to complete the path resolution and continue subsequent processing.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Removing a directory. The centralized coordinator is responsible for removing a directory and invalidating corresponding dentries from all namespace replicas.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In the example shown in Figure 7(c), the client sends an rmdir  $(b)$  request to the coordinator. The coordinator acquires shared locks on all ancestor directories and an exclusive lock on the target directory to ensure path validity during execution. It then forwards the request to the directory inode's owner, i.e.,  $M N o d e_{1}$  in the figure.  $M N o d e_{1}$  locks  $\\mathcal{A}$  's inode to block subsequent lookup requests and broadcasts an invalidation request to other MNodes. Upon receipt, each MNode invalidates its local dentry of the target directory if it exists in the local namespace replica. Each MNode searches its inode table for entries whose key's pid equals to  $\\mathcal{A}$  's ID (i.e.,  $\\mathcal{A}$  's children), and responds to  $M N o d e_{1}$  the existence of any children.  $M N o d e_{1}$  aggregates these responses. If  $\\mathcal{A}$  has no children,  $M N o d e_{1}$  deletes the inode and notifies the coordinator, which will release locks and respond to the client. Otherwise,  $M N o d e_{1}$  returns - ENOTEMPTY to abort the rmdir.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Changing permissions. Operations altering file permissions are also handled by the centralized coordinator in a similar approach. The difference is that the owner MNode will broadcast the invalidation requests and change the permission in its inode table.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Rename. FalconFS ensures rename consistency by employing the central coordination via conventional two- phase locking and two- phase commit protocols. The client will send rename  $(A,B)$  request to the coordinator, who will acquire locks and conduct checks on whether the rename can proceed. Once all checks are passed, the coordinator broadcasts to invalidate dentries of path  $A$  and transfers the inode of  $A$  to the MNode who is responsible for path  $B$",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Locking and conflict resolving. FalconFS leverages namespace replicas to coordinate concurrent file requests. When a server (the coordinator or an MNode) processes a file request, it first resolves the path component by component in its local namespace replica. Dentry locks are acquired during the resolution. To improve parallelism, the coordinator acquires shared locks on intermediate dentries and an exclusive lock on the last component, while MNodes acquire shared locks on all dentries. If a dentry is missing in the local namespace replica, the server locks the dentry after it is retrieved from its owner MNode. Thus, concurrent requests on the same server are serialized by these locks. Concurrent requests on two different MNodes will not incur data race and thus can be executed in parallel. The last case is a request being processed on the coordinator  $(Req_{C})$  and a request on an MNode  $(Req_{M})$",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "For simplicity and without loss of generality, we assume  $Req_{C}$  is removing  $\\mathcal{A} / \\mathcal{B}$  whose inode is on  $M N o d e_{C}$  ,and  $Req_{M}$  is opening  $\\mathcal{A} / \\mathcal{B} / \\mathcal{C}$  on  $M N o d e_{M}$  . During processing  $Req_{C}$ $M N o d e_{C}$  will lock  $\\mathcal{A} / \\mathcal{B}$  's inode and broadcast to invalidate the dentries of  $\\mathcal{A} / \\mathcal{B}$  in all namespace replicas. To handle the invalidation request, an MNode will first lock the corresponding dentry and then mark it as invalid. Then there are two possible cases on  $M N o d e_{M}$",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The first case is that  $Req_{M}$  already holds the lock of  $\\mathcal{A} / \\mathcal{B}$  when the invalidation arrives. In this case, the invalidation will be blocked until  $Req_{M}$  completes, thus  $Req_{C}$  is serialized to happen after  $Req_{M}$",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The second case is that the invalidation is processed before  $Req_{M}$  locks  $\\mathcal{A} / \\mathcal{B}$  In this case,  $M N o d e_{M}$  will find the  $\\mathcal{A} / \\mathcal{B}$  dentry invalid during path resolution of  $Req_{M}$ $M N o d e_{M}$  then sends a lookup request to retrieve the dentry from  $M N o d e_{C}$  who owns the  $\\mathcal{A} / \\mathcal{B}$  inode. On  $M N o d e_{C}$  , the lookup will acquire a shared lock of the  $\\mathcal{A} / \\mathcal{B}$  inode, which has already been locked by  $Req_{C}$  . Thus  $Req_{M}$  will be blocked until  $Req_{C}$  completes, forming a correct serialization of the two requests. Note that when processing the invalidation request,  $M N o d e_{M}$  will discard all lookup responses whose requests are issued before the invalidation is received.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Discussion. Lazy namespace replication introduce one remote lookup for access to non- existent path. However, in DL workloads, such negative access is rare.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4.4 Concurrent Request Merging",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "To further optimize the performance, FalconFS leverages the advantages of stateless- client architecture and adopts concurrent request merging to scale up per- MNode throughput for higher metadata performance. In large- scale deep learning clusters, thousands of compute nodes generate concurrent requests. This presents an opportunity to batch request handling opportunistically, amortizing per- operation overhead - particularly lock contention and write- ahead logging costs.",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Figure 8 illustrates the metadata servers' request- handling mechanism, which employs concurrent request merging. Each MNode initializes a fixed number of database worker threads to serve as the backend for metadata storage and prepares a connection pool. The connection pool accepts incoming",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/6ae5c8a6c80f0f16b90f72dc6f1400b28ea4f71ec8ea66a787b7450ac4cad968.jpg",
        "image_caption": [
            "Figure 8: Concurrent request merging overview."
        ],
        "image_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "requests and puts them into request queues according to the request type. An idle worker thread retrieves a queue and executes all requests in the queue in a single database transaction, with the following optimizations.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Lock coalescing. During path resolution, the worker thread acquires shared locks for all directories along the path to maintain path validity during operations, akin to the implementation of VFS and existing DFS. Prior research demonstrates that lock overhead can be significant even without actual blocking [46]. FalconFS mitigates this through lock coalescing, combining lock acquisition and release operations at per- batch granularity to reduce overhead.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Due to the tree- structured nature of the file system names- . pace, requests can share common near- root path prefixes. As the request queue accumulates multiple operations, the worker coalesces shared path prefixes and eliminates redundant lock acquisitions. In Figure 8, the three create operations each walks two directories and one file. Rather than acquiring nine locks separately, the worker eliminates redundant lock acquisitions and acquires only six locks instead.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Write- ahead- log coalescing. Operations such as mkdir, create, and close modifies the inode table. To maintain file system metadata consistency, DFS typically warp each operation into a separate transaction to persist in atomic [27, 33, 46]. When an transaction commits, it synchronously appends the write- ahead log, leading to small writes that are unfriendly for storage. In FalconFS, as concurrent operations are batched into a single transaction, the worker coalesces small log ap- . pends into larger ones, improving the storage efficiency.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "5 Implementation of VFS Compatibility",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Compatibility with the Linux virtual file system (VFS) is important for easy deployment. However, the VFS embeds path resolution and metadata caching logic within the kernel, which hinders our stateless- client design. To address this, we shortcut VFS path resolution by leveraging the semantics the VFS already provides, enabling users to benefit from FalconFS's design without invasive kernel modifications.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Basic idea. The idea behind VFS shortcut is simple - when the VFS invokes lookup() method provided by the client module for intermediate directories in a path, the method returns directory attributes with permission 0777 to pass VFS checks, and when the VFS triggers the operation on the last path component, the client module sends the full path to the metadata servers, which perform the actual path resolution and execute the requested operation. To implement this approach, two challenges need to be addressed:",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Distinguishing lookup requests to intermediate directories and the final component. The client module returns fake attributes to the former for shortcutting path resolution and real attributes to the latter for correctness, respectively Avoiding fake attributes being exposed to users. A previously returned fake attribute may be cached in the kernel and exposed to users during subsequent operations, which violates correctness and should be avoided.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Distinguish intermediate and final lookups. We observe that existing semantics provided to the lookup() method is sufficient to distinguish lookup intentions. Since Linux kernel 5.7, the VFS sets the global state flag LOOKUP_PARENT during path walk to indicate that the final component has not yet been reached — a feature designed initially for the kernel audit subsystem [6], and the flag is passed to the lookup() method. If the flag is set, the client module knows that the lookup is for an intermediate directory and returns fake attributes (e.g., mode = 0777, along with special uid and gid values) to pass VFS checking.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Avoid exposing fake attributes. To avoid fake attributes being exposed to users due to cache reuse, the client module leverages the VFS d_revalidate() method. We reserve a pair of uid and gid to identify fake attributes. Upon a dcache hit, the VFS invokes d_revalidate() method to validate the cached entry. The client module then checks whether the hit entry is a fake one via uid and gid, and whether the entry is being used to resolve a final path component via the LOOKUP_PARENT flag. If both conditions are met, the module fetches the real attributes from the MNode and updates the dcache entry.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Example. Figure 9 illustrates an example of VFS shortcut. During a getattr operation for the path /a/b, the VFS resolves each component sequentially. It first looks up /, which results in a dcache hit. The VFS invokes the d_revalidate() method to validate the entry, receiving a positive response. Then, the VFS looks up /a, which misses the dcache. The lookup() method is called with the LOOKUP_PARENT flag set, returning a fake attribute. Finally, the VFS looks up /a/b, which also misses the dcache. Here, the lookup() method is invoked without flags. The module then issues a remote lookup request with the full path (/a/b) to the MNode, which executes the real path checking, executes the lookup operation, and returns the result to the module. The module returns the result to the VFS, completing the getattr operation.",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Discussion and limitations. Our client- side implementation preserves path resolution correctness and file system operation integrity, as the MNode re- executes all shortcut checks",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/3f57e26e668a5cb2461cb4f5dfebffb55f6024bab747ce8f1696b2a93b30b951.jpg",
        "image_caption": [
            "Figure 9: Workflow of VFS shortcut in FalconFS. In the figure, The method interfaces and the dcache is simplified for clarity."
        ],
        "image_footnote": [],
        "page_idx": 8
    },
    {
        "type": "table",
        "img_path": "images/c3920b6a7c864ef574282b50573a1ea4fb01a10148b72360525dc5ac869e2e0e.jpg",
        "table_caption": [
            "Table 2: Hardware configuration of the cluster."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>CPU</td><td>2 × Intel Xeon 3.00 GHz, 12 cores</td></tr><tr><td>Memory</td><td>16 × DDR4 2933 MHz 16 GB</td></tr><tr><td>Storage</td><td>2 × NVMe SSD 960 GB</td></tr><tr><td>Network</td><td>2 × 100 GbE</td></tr></table>",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "and prevents user exposure to fake attributes. VFS shortcut has two limitations. First, symbolic link is not supported because the clients do not follow links. Second, nested mount points under FalconFS need special handling (i.e., recheck directory permissions when passing the nested mount point), which is not supported yet.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6 Evaluation",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "We evaluate in this section to present the following results.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "1. FalconFS provides scalable, high-performance metadata operations (§6.2) and file IO (§6.3). \n2. FalconFS's performance is robust under adverse conditions like client memory limitations (§6.4) and load bursts (§6.5). \n3. FalconFS achieves balanced inode distribution across diverse workloads with minimal exception table size (§6.6). \n4. The contribution of each design component to overall performance and the impact of unfavorable conditions (§6.7). \n5. FalconFS's performance in real-world DL workloads (§6.8).",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6.1 Environment Setup",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Testbed. We conduct experiments on a cluster of 13 dual- socket machines (configurations detailed in Table 2). To expand the test scale, we abstract each machine into two independent nodes, with each node bound to one socket, one SSD, and one NIC, scaling the testbed to 26 nodes. We restrict server resources to 4 cores per node to ensure clients can saturate the servers's capability.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Baseline Systems. We compare FalconFS with CephFS 12.2.13 [48], JuiceFS 1.2.1 [3] and Lustre 2.15.6 [4]. CephFS is a widely deployed DFS in data centers. JuiceFS is an open- source DFS targeting AI and data analytics workloads, and we deploy it with TiKV 1.16.1 [8] as its metadata engine and data storage. Lustre is a high- performance DFS widely used in HPC and data centers. CephFS is accessed via the libcephfs library due to observed instability in performance when using a VFS mount point. JuiceFS and Lustre are accessed through",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "VFS mount points, as is FalconFS unless stated otherwise. FalconFS utilizes a modified FUSE kernel module and library that incorporate the optimizations described in §5. All DFSs disable metadata and data replication.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6.2 Metadata Performance",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "We first evaluate the performance of individual metadata operations in the best case, where each client accesses its own private directory and all directory lookups hit the client- side cache. We measure five key metadata operations, namely, create, unlink, getattr, mkdir and rename.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Throughput scalability. We scale the number of metadata servers from 4 to 16 and measure the peak throughput achievable by each file system. To saturate the metadata servers' capacity, we gradually increase the number of client threads until the throughput no longer increases. When mounting with FUSE clients, we observe that 13 client nodes are insufficient to saturate FalconFS, so we present FalconFS's performance using the LibFS interface, enabling each client node to generate higher concurrency. We ensure that the FUSE client and the LibFS client generate identical requests to the metadata servers; thus, given sufficient client nodes, FUSE clients would achieve comparable performance. In production environments, client nodes typically outnumber metadata servers and can fully saturate them. Figure 10 shows the results.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "For the operations create and unlink, FalconFS achieves speedups of  $0.82 - 2.26\\times$  on Lustre and higher gains on CephFS and JuiceFS. This performance improvement stems from two factors: (a) FalconFS do not maintain directory's atime and mtime, eliminating the need to update directory metadata; (b) concurrent request merging consolidates and persists multiple write- ahead- logging operations together, enhancing I/O efficiency. In contrast, JuiceFS and Lustre rely on expensive distributed transactions to update both the file and directory metadata. CephFS do not use distributed transactions but logs writes to remote OSDs — both of which incur significant overhead.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "For getattr, FalconFS achieves  $0.52 - 0.93\\times$  speedup over Lustre. The performance gain comes from that (a) concurrent request merging boosts server concurrency and reduce request dispatching overhead, and (b) FalconFS's stateless- client architecture eliminates the need for acquiring cache coherence locks (e.g., CephFS's capability and Lustre's intent locks).",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "FalconFS demonstrates scalable performance for mkdir, due to efficient invalidation- based synchronization. However, for rmdir, FalconFS's throughput declines as the number of metadata servers increases. This is because rmdir requires invalidating the directory record and querying child inodes across all servers—an overhead proportional to the cluster size. In contrast, CephFS, JuiceFS, and Lustre exhibit constant overhead for rmdir, so their performance is scalable.",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "We observe imbalanced CPU utilization across JuiceFS's metadata engine nodes, indicating inefficient load distribution, which explains JuiceFS's poor performance scalability.",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/15ab8aa521949e094eb052954f19e0c4ca9702c384600f67be63239a350436ce.jpg",
        "image_caption": [
            "Figure 10: Performance and scalability of metadata operations."
        ],
        "image_footnote": [],
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/4c2718f338d51aa507208d4fc1f69817cc3ec4ea78762b3eec2c51241cf46133.jpg",
        "image_caption": [
            "Figure 11: Latency of metadata operations."
        ],
        "image_footnote": [],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Latency. Figure 11 presents the latency of metadata operations across different DFSs. We deployed four metadata servers with a single client thread issuing requests.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Although FalconFS demonstrates superior throughput compared to other DFSs, its latency is higher than Lustre's. This trade- off occurs because FalconFS employs concurrent request merging to batch operations, optimizing throughput at the cost of increased latency. Nevertheless, FalconFS maintains lower latency than both CephFS and JuiceFS, whose heavier software stacks introduce greater overhead.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "6.3 Data Performance",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In this section, we evaluate the performance of small- file access. We deploy four metadata servers and 12 storage nodes, each equipped with one NVMe SSD. We saturate the DFSs using 2560 client threads distributed across 10 client nodes. Each thread accesses 1024 pre- created files within its own private directory. To access a file, a client first opens it with the  $O\\_ DIRECT$  flag, reads or writes all data, and then closes the file. We vary the file size from 4 KiB to 1 MiB and report the normalized throughput in Figure 12.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "When the file size is smaller than  $256\\mathrm{KiB}$ , the throughput increases proportionally with the file size, indicating that the metadata operation IOPS is the bottleneck. When the file size is larger than  $256\\mathrm{KiB}$ , CephFS, Lustre and FalconFS's throughput hits the SSD bandwidth bottleneck of  $43\\mathrm{GiB / s}$  for read and  $16\\mathrm{GiB / s}$  for write. Thanks to FalconFS's higher metadata performance, it outperforms other DFSs in small- file access. For files no larger than  $64\\mathrm{KiB}$ , FalconFS achieves  $7.35 - 21.23\\times$  speedup over CephFS,  $0.86 - 24.87\\times$  over JuiceFS and  $1.12 - 1.85\\times$  over Lustre. JuiceFS does not perform well in small file access due to the inefficiency of the data storage storage.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "6.4 Impact of Client Memory Budget",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In this section, we evaluate the impact of client memory budget on DFS performance under typical DL training workloads - specifically, random file traversal in a large directory tree. We initialize an 8- level directory tree structure where each intermediate directory contains ten subdirectories and each last- level directory contains ten 64 KiB files. This configuration yields a total of 11.1 million directories and 100 million files. Each DFS runs four metadata servers and twelve storage nodes. Ten client nodes, each running a 256- thread client process, read all files in independent random orders.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We limit the metadata cache size on each client node based on the ratio of the total size of all directories' inodes and dentries. For CephFS, we set the ceph.conf parameter client_cache_size to enforce this limit. For other DFSs, we use Control Group v2 to restrict the cache size. The cgroup monitors the process's userspace and kernel memory usage, and reclaims kernel objects (e.g., dentries and inodes) when memory consumption exceeds the threshold.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In addition to CephFS and Lustre, we evaluate FalconFS- NoBypass, a variant of FalconFS without the VFS shortcut, to highlight the benefit of the client- stateless design. FalconFS- NoBypass relies on the VFS dentry and inode caches to perform client- side path resolution. We omit the results for JuiceFS, as its throughput drops to zero before completing the initialization of the directory tree.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Figure 13(a) shows the throughput of each DFS under different memory budgets and Figure 13(b) presents the composition of metadata requests. Notably, while Lustre and FalconFS explicitly send open requests to open files, CephFS sends lookup requests for file open. For simplicity, we count CephFS's lookup requests to files as open in Figure 13(b).",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We make the following observations: First, the performance of stateful- client DFSs, including CephFS, Lustre and FalconFS- NoBypass, is sensitive to the client memory budget. There is a  $1.4 - 1.5\\times$  performance gap between the  $10\\%$  and  $100\\%$  memory budget configurations. When the memory budget is constrained, fewer directories can be cached on the client- side, leading to more frequent lookups, which increase the number of requests per file access and degrade throughput.",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Second, FalconFS achieves consistently high performance even under tight memory budget and outperforms FalconFS- NoBypass, demonstrating that stateless- client design effectively boosts performance. As shown in Figure 13(b), FalconFS generates constant number of requests to the meta-",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/11bd0f9107d4daa742ce1ede2f9f76fdef1a909ea20292c9ddf82635319814ac.jpg",
        "image_caption": [
            "Figure 12: Throughput of file data IO. Y-axis is the throughput normalized to that of FalconFS."
        ],
        "image_footnote": [],
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/7057151db68342d525fb3b0e6d17333cb380c27ef9b144d3d344c152056d8136.jpg",
        "image_caption": [
            "Figure 13: Random file traversal in a large directory tree."
        ],
        "image_footnote": [],
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/d5d9b90db52b2640dfcf0fa96c7f5ee1caf4f61d53ee90c4bae00f7327667598.jpg",
        "image_caption": [
            "Figure 14: Throughput of burst file IO."
        ],
        "image_footnote": [],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "data servers as cache size varies. Compared with FalconFS- NoBypass, FalconFS reduce the number of metadata requests by  $22.7\\% - 45.9\\%$  and improves the throughput by  $0.24 - 0.94\\times$ . Compared with CephFS and Lustre, FalconFS improve the throughput by  $2.92 - 4.72\\times$  and  $2.08 - 3.34\\times$  respectively.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "6.5 Impact of Burst IO",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We evaluate the impact of directory bursts on DFS performance, a common access pattern in DL labeling workloads. We deploy four metadata servers and 12 storage nodes, and uses a single 256- thread client node to access pre- created 64 KiB files in bursts. A burst is defined as a sequence of accesses to files within the same directory, with adjacent bursts targeting different directories. Figure 14 presents the results.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We observe performance degradation in CephFS's read and write operations, as well as Lustre's read performance, as the burst size increases. This occurs because large bursts cause instantaneous load imbalance across the metadata servers. In contrast, FalconFS does not suffer from large bursts, as it evenly distributes the metadata of files within the same directory, achieving good scalability. JuiceFS's performance also does not degrade with increasing burst size, as there is a constant load imbalance among its metadata engine nodes.",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/8dcb4fcf72fb135a037f7b0a7e4b79e26d55ead73a4f1fbcaa7bf8e5c7152856.jpg",
        "table_caption": [
            "Table 3: File and directory inode distribution of various directory structures over 16 metadata servers."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>inode #</td><td>inode distribution max</td><td>min</td><td colspan=\"2\">exception entry # path-walk overriding</td></tr><tr><td>Labeling task</td><td>33320</td><td>6.99%</td><td>5.30%</td><td>0</td><td>0</td></tr><tr><td>ImageNet [17]</td><td>2027728</td><td>6.29%</td><td>6.21%</td><td>0</td><td>0</td></tr><tr><td>KITTI [13]</td><td>15003</td><td>7.01%</td><td>5.47%</td><td>0</td><td>0</td></tr><tr><td>Cityscapes [11]</td><td>20022</td><td>6.30%</td><td>6.22%</td><td>0</td><td>0</td></tr><tr><td>CelebA [26]</td><td>202599</td><td>6.54%</td><td>6.95%</td><td>0</td><td>0</td></tr><tr><td>SVHN [32]</td><td>33404</td><td>6.77%</td><td>5.76%</td><td>0</td><td>0</td></tr><tr><td>CUB-200-2011 [44]</td><td>12003</td><td>6.68%</td><td>5.95%</td><td>0</td><td>0</td></tr><tr><td>Linux-6.8 code</td><td>88936</td><td>6.49%</td><td>5.96%</td><td>2</td><td>0</td></tr><tr><td>FSL homes [40]</td><td>655177</td><td>6.83%</td><td>5.45%</td><td>1</td><td>0</td></tr></table>",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "6.6 Load Balance in Real Workloads",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In this section, we demonstrate that hybrid metadata indexing achieves a balanced distribution of inodes across diverse directory structures, with only a small portion of filenames requiring special treatment. We evaluate inode distribution on both DL workloads and general- purpose workloads. For DL workloads, we analyze a dataset collected from Huawei's production environment, as well as six popular open- source image datasets used in deep learning. We select these open- source datasets by listing the most popular image datasets on a dataset summary website [5], and choose the first six datasets containing more than 10,000 files. For general- purpose workloads, we select the Linux 6.8 source tree and FSL home traces [40], the latter being a snapshot of students' home directories from a shared NFS on a university campus.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Table 3 summarizes, for each workload, the number of files, the maximum and minimum ratios of inodes on metadata servers, and the number of exception entries utilized to achieve the distribution. As shown in the table, most workloads exhibit a small max- min gap with zero exception entries, indicating that filename hashing alone is sufficient to achieve balanced inode distribution in these cases. This is because such workloads — typically datasets — have large directory size, which is friendly to filename hashing.",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We further examine the three workloads that require exception entries. The \"Linux 6.8 code tree\" contains many files with identical names; however, applying path- walk redirection to the two most frequent filenames (i.e., \"Makefile\" and \"Kconfig\") suffices to balance the distribution. These two",
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/73d52ab04bd9497efcd4db84700f17868c3ad9103021f13a29b506988b71703e.jpg",
        "image_caption": [
            "Figure 15: Performance analysis."
        ],
        "image_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "filenames occur 2,945 and 1,690 times respectively, accounting for  $5.55\\%$  of all files in total. In the FSL home traces, FalconFS achieves balanced load after applying path- walk redirection to the most frequent filename, which appears 8,112 times and accounts for  $1.24\\%$  of all files.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "6.7 Performance Analysis",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Design contributions. The contribution of the stateless- client architecture to overall performance is demonstrated by comparing FalconFS with FalconFS- NoBypass in  $\\S 6.4$  .In this experiment, we further analyze other design configurations by evaluating three setups: the full FalconFS, no inv, and no merge, incrementally reducing the design features. The no inv configuration disables invalidation- based synchronization, wrapping mkdir operations in a distributed transaction to atomically create all dummy replicas across all MNodes. The no merge setup disables concurrent request merging in addition to the changes in no inv, requiring worker threads to fetch and execute requests one a time. For this evaluation, we deploy four MNodes and use LibFS clients to saturate them, with each client accessing its own private directory. Figure 15(a) presents the peak throughput of the mkdir operation.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Compared to the full FalconFS, no inv decreases throughput by  $86.9\\%$  , as the distributed transaction requires multiple rounds of broadcasts, incurring significant overhead. no merge reduces throughput by an additional  $91.8\\%$  , as request dispatching from the connection pool to the worker threads becomes a bottleneck due to shared request queue contention.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Corner case analysis. In most cases, hybrid metadata indexing enables one- hop operations. However, there are corner cases that require two hops: (a) operations on non- existent paths, (b) operations on path- walk redirected filenames, and (c) operations issued with a stale exception table. Figure 15(b) illustrates how these scenarios affect the performance of the getattr operation. Compared to the one- hop common case, these corner cases result in a  $36.8\\% - 49.6\\%$  decrease in performance due to the additional hop.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "6.8 End-to-End Performance",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In this section, we evaluate the end- to- end performance of FalconFS in DL workloads for both labeling and training tasks. For the labeling task, we replay a trace captured from a production environment. In this trace, labeling tasks read raw images from the DFS and write segmented images back to the DFS. Figure 16(a) shows the distribution of file sizes in the trace, and Figure 16(b) presents the completion time of the trace replay. Although we do not replay GPU computation, the measured completion time closely approximates the end- to- end runtime, as computation overlaps with I/O and I/O remains the bottleneck. Compared to other DFSs, FalconFS reduces the runtime by  $23.8\\% - 86.4\\%$",
        "page_idx": 11
    },
    {
        "type": "image",
        "img_path": "images/89d5a4578f8197657776b2ace944e0611d41b6345a972962a52920018b94d4ce.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 11
    },
    {
        "type": "image",
        "img_path": "images/3dde23ff1ee0d33d7b89754d6a54fb5349fd950f263bfc7c23d37f7172f32b4c.jpg",
        "image_caption": [
            "Figure 16: File size pattern and runtime for labeling task replay.",
            "Figure 17: Accelerator utilization for Resnet-50 model training."
        ],
        "image_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "For the training task, we evaluate DFS performance using the MLPerf Storage Benchmark [28]. The benchmark is configured to simulate training a ResNet- 50 model on 10 million files distributed across 1 million directories, with each file sized at  $112\\mathrm{KiB}$  using varying numbers of GPUs. The total dataset size is 1,064 GiB, and files are read using direct I/O. Figure 17 shows the accelerator utilization (AU) of each DFS as the number of GPUs increases. JuiceFS is omitted because its throughput drops to zero during dataset initialization. Taking  $90\\%$  AU as the threshold, FalconFS supports up to 80 GPUs, while Lustre supports only 32 GPUs, and CephFS does not meet the threshold. With 80 to 128 GPUs, FalconFS achieves training throughput speedups of  $11.09 - 11.81\\times$  over CephFS and  $0.99 - 1.23\\times$  over Lustre.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "7 Related Works",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Path Resolution Optimizations. Path resolution overhead has drawn research attention for a long time. In the context of local file systems, a series of studies propose optimizations like full- path indexing [10,23,43] and VFS modifications [47]. These approaches optimize local data structures and cannot be directly applied to distributed file systems (DFSs).",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "To accelerate path resolution, DFSs typically adpots clientside metadata caching [4,15,25,27,36,37,48]. InfiniFS [27] reduces the latency penalty of cache misses by resolving multiple path components in parallel; however, it cannot mitigate request amplification. Giraffa [39] and CalvinFS [41] locate file inodes by full path hashing, which makes directory renaming hard to implement. HDFS [38] performs path resolution on a centralized namenode and thus has scalability issues. HopsFS [33] performs path resolution at a proxy layer, which looks up all path components in parallel from a distributed database, but still suffers from constant request amplification.",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Our approach differs in that FalconFS addresses scalability, request amplification and metadata indexing issues through a client- stateless architecture and hybrid metadata indexing.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Storage Systems for Deep Learning. Previous studies propose various data loading frameworks for DL training tasks [19, 21, 22, 29, 50]. These works optimize DL data loading by unifying data access, reusing pre- processed data, and leveraging data caching, etc. These works are task- specific and can be deployed on top of FalconFS. DIESEL [45] is a DFS designed for DL training tasks. Its design is based on the assumption that the dataset's directory tree is small enough to be cached on every client, whereas FalconFS makes the opposite assumption and focuses on eliminating client- side caching. 3FS [9] is a recent DFS for AI workloads. Unlike FalconFS, it is optimized for large data access and focuses on optimizing the data path, while FalconFS optimizes the metadata architecture for small- file performance.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "8 Conclusion",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "We propose FalconFS, a distributed file system with a client- stateless architecture for DL workloads. Evaluations show that FalconFS achieves up to  $4.72\\times$  better throughput of small file random access and up to  $11.81\\times$  higher GPU utilization in deep learning model training over CephFS and Lustre.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "References",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[1] Building a data pipeline for deep learning. https: //www.netapp.com/pdf.html?item  $=$  /media/6750 - wp- 7264. pdf. [2] Fire- flyer file system - design notes. https://github .com/deepseek- ai/3FS/blob/main/docs/design_ notes.md. [3] A high- performance, cloud- native, distributed file system. https://juicefs.com/en/. [4] Lustre filesystem. https://www.lustre.org/. [5] Machine learning datasets | papers with code. https: //paperswithcode.com/datasets?mod  $\\equiv$  images. Accessed April 9, 2025. [6] Pathname lookup. https://docs.kernel.org/filesystems/path- lookup.html. Accessed April 9, 2025. [7] Postgresql: The world's most advanced open source relational database. https://www.postgresql.org/. [8] Tikv is a highly scalable, low latency, and easy to use key- value database. https://tikv.org//. Accessed April 9, 2025. [9] Wei An, Xiao Bi, Guanting Chen, Shanhuang Chen, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Wenjun Gao, Kang Guan, Jianzhong Guo, Yongqiang",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Guo,Zhe Fu, Ying He, Panpan Huang, Jiashi Li, Wenfeng Liang, Xiaodong Liu, Xin Liu, Yiyuan Liu, Yuxuan Liu, Shanghao Lu, Xuan Lu, Xiaotao Nie, Tian Pei, Junjie Qiu, Hui Qu, Zehui Ren, Zhangli Sha, Xuecheng Su, Xiaowen Sun, Yixuan Tan, Minghui Tang, Shiyu Wang, Yaohui Wang, Yongji Wang, Ziwei Xie, Yiliang Xiong, Yanhong Xu, Shengfeng Ye, Shuiping Yu, Yukun Zha, Liyue Zhang, Haowei Zhang, Mingchuan Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, and Yuheng Zou. Fire- flyer al- npc: A cost- effective software- hardware co- design for deep learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis, SC '24. IEEE Press, 2024.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[10] Miao Cai, Junru Shen, Bin Tang, Hao Huang, and Baoliu Ye. FlatFS: Flatten hierarchical file system namespace on non- volatile memories. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages 899- 914, Carlsbad, CA, July 2022. USENIX Association.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[12] Bin Fan, Hyeontaek Lim, David G. Andersen, and Michael Kaminsky. Small cache, big effect: provable load balancing for randomly partitioned cluster services. In Proceedings of the 2nd ACM Symposium on Cloud Computing, SOCC '11, New York, NY, USA, 2011. Association for Computing Machinery.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[13] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[14] Gluster. Storage for your cloud. https://www.gluster.org/, 2019.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[15] ThinkParQ GmbH. Beegfs documentation 7.4.2. https://doc.beegfs.io/latest/index.html. Accessed November 17, 2023.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[16] Dan Graur, Damien Aymon, Dan Kluser, Tanguy Albrici, Chandramohan A. Thekkath, and Ana Klimovic. Cachew: Machine learning input data processing as a service. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages 689- 706, Carlsbad, CA, July 2022. USENIX Association.",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "[17] Addison Howard, Eunbyung Park, and Wendy Kan. Imagenet object localization challenge. https://kaggle",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": ".com/competitions/imagenet- object- localiz ation- challenge, 2018. Kaggle.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[18] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster, Changhoon Kim, and Ion Stoica. Netcache: Balancing key- value stores with fast in- network caching. In Proceedings of the 26th Symposium on Operating Systems Principles, SOSP '17, pages 121- 136, New York, NY, USA, 2017. Association for Computing Machinery.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[19] Aarati Kakarapartny, Abhay Venkatesh, Amar Phanishayee, and Shivaram Venkataraman. The case for unifying data loading in machine learning clusters. In 11th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 19), Renton, WA, July 2019. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[20] Antonios Katsarakis, Vasilis Gavrielatos, M.R. Siavash Katebzadeh, Arpit Joshi, Aleksandar Dragojevic, Boris Grot, and Vijay Nagarajan. Hermes: A fast, fault- tolerant and linearizable replication protocol. In Proceedings of the Twenty- Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '20, page 201- 217, New York, NY, USA, 2020. Association for Computing Machinery.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[21] Abhishek Vijaya Kumar and Muthian Sivathamu. Quiver: An informed storage cache for deep learning. In 18th USENIX Conference on File and Storage Technologies (FAST 20), pages 283- 296, Santa Clara, CA, February 2020. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[22] Gyewon Lee, Irene Lee, Hyeonmin Ha, Kyunggeun Lee, Hwarim Hyun, Ahrajae Shin, and Byung- Gon Chun. Refurbish your training data: Reusing partially augmented samples for faster deep neural network training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 537- 550. USENIX Association, July 2021.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[23] Paul Hermann Lensing, Toni Cortes, and Andre Brinkmann. Direct lookup and hash- based metadata placement for local file systems. In Proceedings of the 6th International Systems and Storage Conference, SYSTOR '13, New York, NY, USA, 2013. Association for Computing Machinery.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[24] Qiang Li, Qiao Xiang, Yuxin Wang, Haohao Song, Ridi Wen, Wenhui Yao, Yuanyuan Dong, Shuqi Zhao, Shuo Huang, Zhaosheng Zhu, Huayong Wang, Shanyang Liu, Lulu Chen, Zhiwu Wu, Haonan Qiu, Derui Liu, Gexiao Tian, Chao Han, Shaozong Liu, Yaohui Wu, Zicheng Luo, Yuchao Shao, Junping Wu, Zheng Cao, Zhongjie Wu, Jiaji Zhu, Jinbo Wu, Jiwu Shu, and Jiesheng Wu. More than capacity: Performance- oriented evolution of pangu in alibaba. In 21st USENIX Conference on File and Storage Technologies (FAST 23), pages 331- 346, Santa Clara, CA, February 2023. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "and Storage Technologies (FAST 23), pages 331- 346, Santa Clara, CA, February 2023. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[25] Siyang Li, Youyou Lu, Jiwu Shu, Yang Hu, and Tao Li. Locofs: A loosely- coupled metadata service for distributed file systems. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '17, New York, NY, USA, 2017. Association for Computing Machinery.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[27] Wenhao Lv, Youyou Lu, Yiming Zhang, Peile Duan, and Jiwu Shu. InfiniFS: An efficient metadata service for Large- Scale distributed filesystems. In 20th USENIX Conference on File and Storage Technologies (FAST 22), pages 313- 328, Santa Clara, CA, February 2022. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[28] MLCommons. Mlperf storage benchmark suite. https://github.com/mlcommons/storage. Accessed April 9, 2025.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[29] Jayashree Mohan, Amar Phanishayee, Ashish Raniwala, and Vijay Chidambaram. Analyzing and mitigating data stalls in dnn training. Proc. VLDB Endow., 14(5):771- 784, January 2021.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[30] Derek G. Murray, Jiří Šimša, Ana Klimovic, and Ihor Indyk. tf.data: a machine learning data processing framework. Proc. VLDB Endow., 14(12):2945- 2958, July 2021.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[31] NetApp. How to build a data pipeline for autonomous driving. https://www.netapp.com/blog/how- to- build- a- data- pipeline- for- autonomous- driving/. Accessed April 9, 2025.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[32] Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[33] Salman Niazi, Mahmoud Ismail, Seif Haridi, Jim Dowling, Steffen Grohsschmiedt, and Mikael Ronström. HopsFS: Scaling hierarchical file system metadata using NewSQL databases. In 15th USENIX Conference on File and Storage Technologies (FAST 17), pages 89- 104, Santa Clara, CA, February 2017. USENIX Association.",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "[34] Satadru Pan, Theano Stavrinos, Yunqiao Zhang, Atul Sikaria, Pavel Zakharov, Abhinav Sharma, Shiva Shankar P, Mike Shuey, Richard Wareing, Monika Gangapuram, Guanglei Cao, Christian Preseau, Pratap Singh, Kestutis Patiejunas, JR Tipton, Ethan Katz- Bassett, and Wyatt Lloyd. Facebook's tectonic",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "filesystem: Efficiency from exascale. In 19th USENIX Conference on File and Storage Technologies (FAST 21), pages 217- 231. USENIX Association, February 2021.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[35] Shi Qiu, Weinan Liu, Yifan Hu, Jianqin Yan, Zhirong Shen, Xin Yao, Renhai Chen, Gong Zhang, and Yiming Zhang. GeminiFS: A companion file system for GPUs. In 23rd USENIX Conference on File and Storage Technologies (FAST 25), pages 221- 236, Santa Clara, CA, February 2025. USENIX Association.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[36] Kai Ren, Qing Zheng, Swapnil Patil, and Garth Gibson. Indexfs: Scaling file system metadata performance with stateless caching and bulk insertion. In SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 237- 248, 2014.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[37] S. Shepler, B. Callaghan, D. Robinson, R. Thurlow, Inc. Sun Microsystems, C. Beame, Hummingbird Ltd., M. Eisler, D. Noveck, and Inc. Network Appliance. Network file system (nfs) version 4 protocol. https://www.ietf.org/rfc/rfc3530. txt, 2003.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[38] Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. The hadoop distributed file system. In 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), pages 1- 10, 2010.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[39] Konstantin V Shvachko and Yuxiang Chen. Scaling namespace operations with giraffa file system. USENIX; login, 2017.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[40] Vasily Tarasov, Amar Mudrankit, Will Buik, Philip Shi- lane, Geoff Kuenning, and Erez Zadok. FSL- dedup traces (SNIA IOTTA trace set 5228). In Geoff Kuenning, editor, SNIA IOTTA Trace Repository. Storage Networking Industry Association, May 2016.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[41] Alexander Thomson and Daniel J. Abadi. CalvinFS: Consistent WAN replication and scalable metadata management for distributed file systems. In 13th USENIX Conference on File and Storage Technologies (FAST 15), pages 1- 14, Santa Clara, CA, February 2015. USENIX Association.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[42] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Suresh Anthony, Hao Liu, Pete Wyckoff, and Raghotham Murthy. Hive: a warehousing solution over a map- reduce framework. Proc. VLDB Endow., 2(2):1626- 1629, August 2009.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[43] Chia- Che Tsai, Yang Zhan, Jayashree Reddy, Yizheng Jiao, Tao Zhang, and Donald E. Porter. How to get more value from your file system directory cache. In Proceedings of the 25th Symposium on Operating Systems",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Principles, SOSP '15, page 441- 456, New York, NY, USA, 2015. Association for Computing Machinery.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[44] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech- ucsd birds- 200- 2011 (cub- 200- 2011). Technical Report CNS- TR- 2011- 001, California Institute of Technology, 2011.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[45] Lipeng Wang, Songgao Ye, Baichen Yang, Youyou Lu, Hequan Zhang, Shengen Yan, and Qiong Luo. Diesel: A dataset- based distributed storage and caching system for large- scale deep learning training. In Proceedings of the 49th International Conference on Parallel Processing, ICPP '20, New York, NY, USA, 2020. Association for Computing Machinery.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[46] Yiduo Wang, Yufei Wu, Cheng Li, Pengfei Zheng, Biao Cao, Yan Sun, Fei Zhou, Yinlong Xu, Yao Wang, and Guangjun Xie. CFS: Scaling metadata service for distributed file system via pruned scope of critical sections. In Proceedings of the Eighteenth European Conference on Computer Systems, EuroSys '23, page 331- 346, New York, NY, USA, 2023. Association for Computing Machinery.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[47] Ying Wang, Dejun Jiang, and Jin Xiong. Caching or not: Rethinking virtual file system for Non- Volatile main memory. In 10th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 18), Boston, MA, July 2018. USENIX Association.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[48] Sage A. Weil, Scott A. Brandt, Ethan L. Miller, Darrell D. E. Long, and Carlos Maltzahn. Ceph: A scalable, high- performance distributed file system. In Proceedings of the 7th Symposium on Operating Systems Design and Implementation, OSDI '06, pages 307- 320, USA, 2006. USENIX Association.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[49] Matei A. Zaharia, Ali Ghodsi, Reynold Xin, and Michael Armbrust. Lakehouse: A new generation of open platforms that unify data warehousing and advanced analytics. In Conference on Innovative Data Systems Research, 2021.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[50] Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Mingxia Li, Fan Yang, Qianxi Zhang, Binyang Li, Yuqing Yang, Lili Qiu, Lintao Zhang, and Lidong Zhou. Silod: A co- design of caching and scheduling for deep learning clusters. In Proceedings of the Eighteenth European Conference on Computer Systems, EuroSys '23, page 883- 898, New York, NY, USA, 2023. Association for Computing Machinery.",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "[51] Mark Zhao, Niket Agarwal, Aarti Basant, Bugra Gedik, Satadru Pan, Mustafa Ozdal, Rakesh Komuravelli, Jerry Pan, Tianshu Bao, Haowei Lu, Sundaram Narayanan,",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Jack Langman, Kevin Wilfong, Harsha Rastogi, Carole- Jean Wu, Christos Kozyrakis, and Parik Pol. Understanding data storage and ingestion for large- scale deep recommendation model training: Industrial product. In Proceedings of the 49th Annual International Symposium on Computer Architecture, ISCA '22, pages 1042- 1057, New York, NY, USA, 2022. Association for Computing Machinery.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "[52] Qing Zheng, Charles D. Cranor, Gregory R. Ganger, Garth A. Gibson, George Amvrosiadis, Bradley W. Settlemeyer, and Gary A. Grider. Deltas: a scalable no- ground- truth file system for massively- parallel computing. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '21, New York, NY, USA, 2021. Association for Computing Machinery.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A Appendix",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A.1 Theoretical Analysis",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We demonstrate that hybrid metadata indexing (§4.2) achieves an even distribution of inodes with at most  $O(n\\log n)$  exception table entries — not only for DL workloads but also for arbitrary directory structures, where  $n$  denotes the number of MNodes. We start our discussion with strong assumptions on the directory structure and progressively relax them.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Many filenames, identical frequency. First, we assume that the file system namespace contains significantly more unique filenames than MNodes, with each filename appearing an equal number of times. Under this condition, filename hashing ensures a statistically even distribution of inodes across MNodes, as dictated by the law of large numbers.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Many filenames, varying frequency. Then we remove the assumption that all filenames appear with equal frequency. We demonstrate that by applying path- walk redirection to the  $O(n\\log n)$  most frequent filenames and applying filename hashing to the remainder, an even distribution of inodes across MNodes can be achieved—regardless of the underlying filename frequency distribution.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Our proof builds upon a theorem from caching literature [12, 18], which states: for  $m$  objects randomly partitioned across  $n$  nodes with a total query load of  $n \\cdot t$ , if a cache absorbs all queries to the hottest  $O(n\\log n)$  items, then no node exceeds  $t$  load with high probability, independent of the query distribution. We adapt this through constructive proof.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Consider  $n \\cdot t$  files with  $m$  distinct filenames, randomly partitioned across  $n$  nodes via filename hashing, and a query load accessing each file uniformly. Now we think of the filenames as the objects in the theorem. The query load on each filename is proportional to the number of files with that filename. The theorem guarantees that after removing queries for the hottest  $O(n\\log n)$  filenames, the remaining load is evenly distributed across nodes. It indicates that files not among these hottest  $O(n\\log n)$  filenames must themselves be evenly distributed across nodes.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Now that the theorem guarantees that files not among the hottest  $O(n\\log n)$  filenames are evenly distributed across nodes and that we apply path- walk redirection to the  $O(n\\log n)$  most frequent filenames to ensure their even distribution, the entire namespace must be evenly distributed.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A few filenames, varying frequency. Finally, we relax the assumption that filenames significantly outnumber MNodes, considering instead the case where only  $O(n)$  distinct file- names exist in the namespace. A trivial solution for achieving even inode distribution with at most  $O(n)$  exception entries would be to apply path- walk redirection to all filenames, thus completing our theoretical proof.",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In practice, we avoid path- walk redirection since it introduces an additional hop for file operations. Instead, our load balancing algorithm (§4.2.2) prioritizes overriding redirect-",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "tion over path- walk redirection, resorting to the latter only when necessary.",
        "page_idx": 16
    }
]